{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from vae import DisentangleVAE\n",
    "from dataset import CelebADataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = '../results'\n",
    "DATA_DIR = '../data'\n",
    "\n",
    "IMG_SIZE = 64\n",
    "NB_CHANNELS = 3\n",
    "Z_DIM = 64\n",
    "BETA = 1\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b5f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_dm = CelebADataModule(\n",
    "    data_dir = DATA_DIR,\n",
    "    img_size = IMG_SIZE,\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n",
    "celeba_dm.prepare_data()\n",
    "celeba_dm.setup()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08164837",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE = DisentangleVAE(\n",
    "    img_size = IMG_SIZE,\n",
    "    nb_channels = NB_CHANNELS,\n",
    "    z_dim = Z_DIM,\n",
    "    beta = BETA,\n",
    "    learning_rate = LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpooint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath = os.path.join(RESULTS_DIR, 'checkpoints'),\n",
    "    filename = 'disentangle_vae-{epoch:02d}-{val_loss:.2f}',\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    save_top_k = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs = EPOCHS,\n",
    "    accelerator = 'auto',\n",
    "    devices = 'auto',\n",
    "    callbacks = [checkpooint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(VAE, datamodule = celeba_dm)\n",
    "trainer.save_checkpoint(os.path.join(RESULTS_DIR, 'disentangle_vae.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading best model for inference...\")\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "if best_model_path:\n",
    "    loaded_model = VAE.load_from_checkpoint(best_model_path)\n",
    "else:\n",
    "    loaded_model = vae_model # Fallback\n",
    "\n",
    "loaded_model.eval()\n",
    "# Sposta il modello sul device corretto per l'inferenza\n",
    "device = trainer.strategy.root_device # Ottieni il device su cui il trainer ha addestrato\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Genera nuove immagini\n",
    "print(\"Generating new images with the trained model...\")\n",
    "with torch.no_grad():\n",
    "    sample = torch.randn(16, loaded_model.z_dim).to(device)\n",
    "    generated_images = loaded_model._decode(sample).cpu()\n",
    "    from torchvision.utils import make_grid, save_image # Importa qui se non usi utils.py\n",
    "    grid = make_grid(generated_images, nrow=4, padding=2, normalize=True)\n",
    "    save_image(grid, os.path.join(RESULTS_DIR, 'final_generated_samples.png'))\n",
    "    print(f\"Final generated samples saved in {os.path.join(RESULTS_DIR, 'final_generated_samples.png')}\")\n",
    "\n",
    "# Ricostruisci immagini di esempio\n",
    "print(\"\\nReconstructing example images from the dataset...\")\n",
    "with torch.no_grad():\n",
    "    # Ottieni un batch dal dataloader di validazione\n",
    "    # Non è necessario chiamare dm.setup() di nuovo se trainer.fit() è già stato chiamato\n",
    "    # e dm.val_dataloader() è disponibile\n",
    "    val_dataloader_for_inference = celeba_dm.val_dataloader() \n",
    "    data_iter = iter(val_dataloader_for_inference)\n",
    "    sample_batch, _ = next(data_iter)\n",
    "    sample_batch = sample_batch[:16].to(device) # Prendi le prime 16 immagini\n",
    "    \n",
    "    recon_sample_batch, _, _ = loaded_model(sample_batch)\n",
    "    \n",
    "    comparison = torch.cat([sample_batch.cpu(), recon_sample_batch.cpu()])\n",
    "    grid_comparison = make_grid(comparison, nrow=16, padding=2, normalize=True)\n",
    "    save_image(grid_comparison, os.path.join(RESULTS_DIR, 'final_reconstructions_comparison.png'))\n",
    "    print(f\"Reconstruction comparison saved in {os.pi.join(RESULTS_DIR, 'final_reconstructions_comparison.png')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae-compare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
